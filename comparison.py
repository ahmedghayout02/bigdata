import pandas as pd
import dask.dataframe as dd
import time
import os
import psutil
import pyarrow as pa
import pyarrow.parquet as pq
from dask.distributed import Client
import sys
import gc


sys.stdout.reconfigure(encoding='utf-8')

# ğŸ“Œ Ù‚ÙŠØ§Ø³ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø©

def measure_memory(repeats=10, delay=0.5):  # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª ÙˆØ§Ù„ØªØ£Ø®ÙŠØ±
    """Ù‚ÙŠØ§Ø³ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¨Ø¯Ù‚Ø© Ø¨Ø£Ø®Ø° Ù…ØªÙˆØ³Ø· Ø¹Ø¯Ø© Ù‚Ø±Ø§Ø¡Ø§Øª"""
    readings = []
    process = psutil.Process(os.getpid())
    for _ in range(repeats):
        mem = process.memory_info().rss / (1024 ** 2)  # Ø¨Ø§Ù„Ù…ÙŠØ¬Ø§Ø¨Ø§ÙŠØª
        readings.append(mem)
        time.sleep(delay)
    return max(readings)  # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø·


# ğŸ”„ ØªØ­ÙˆÙŠÙ„ CSV Ø¥Ù„Ù‰ Parquet Ù…Ø¹ Ø§Ù„Ø¶ØºØ·
def convert_to_parquet(input_file: str, output_file: str, chunk_size: int = 5000) -> bool:
    """ÙŠØ­ÙˆÙ‘Ù„ Ø§Ù„Ù…Ù„Ù CSV Ø¥Ù„Ù‰ Parquet Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡"""
    if not os.path.exists(output_file):
        print(f"ğŸ”„ Ø¬Ø§Ø±ÙŠ ØªØ­ÙˆÙŠÙ„ {os.path.basename(input_file)} Ø¥Ù„Ù‰ Parquet...")
        writer = None
        try:
            for i, chunk in enumerate(pd.read_csv(input_file, chunksize=chunk_size, low_memory=False)):
                chunk = chunk.apply(lambda x: x.astype(str) if x.dtype == object else x)
                
                table = pa.Table.from_pandas(chunk)
                
                if not writer:
                    writer = pq.ParquetWriter(output_file, schema=table.schema, compression='snappy', use_dictionary=True)
                
                writer.write_table(table)
                print(f"âœ… ØªÙ… ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¬Ø²Ø¡ {i+1}")
            
            print(f"ğŸ‰ Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Parquet Ø§ÙƒØªÙ…Ù„ Ø¨Ù†Ø¬Ø§Ø­! Ø§Ù„Ù…Ù„Ù: {output_file}")
            return True
        except Exception as e:
            print(f"âŒ ÙØ´Ù„ Ø§Ù„ØªØ­ÙˆÙŠÙ„: {str(e)}")
            if writer:
                writer.close()
                os.remove(output_file)
            return False
        finally:
            if writer:
                writer.close()
    else:
        print(f"âš ï¸ Ø§Ù„Ù…Ù„Ù {output_file} Ù…ÙˆØ¬ÙˆØ¯ Ù…Ø³Ø¨Ù‚Ù‹Ø§")
        return True

# ğŸ“Š Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Pandas (ØªØ¬Ø²Ø¦Ø©)
def pandas_chunking(file_name, chunk_size=10000):
    start_mem = measure_memory()
    start_time = time.time()

    total_sum = {}
    total_count = {}

    try:
        for chunk in pd.read_csv(file_name, chunksize=chunk_size, low_memory=False):
            grouped = chunk.groupby("event_type")["price"]
            for name, group in grouped:
                total_sum[name] = total_sum.get(name, 0) + group.sum()
                total_count[name] = total_count.get(name, 0) + group.count()

        final_result = {name: total_sum[name] / total_count[name] for name in total_sum}
        
        return {
            'Method': 'Pandas Chunking',
            'Time (Seconds)': round(time.time() - start_time, 2),
            'Memory (MB)': round(measure_memory() - start_mem, 2)
        }
    except Exception as e:
        print(f"âŒ ÙØ´Ù„ ÙÙŠ Pandas Chunking: {e}")
        return {'Method': 'Pandas Chunking', 'Time (Seconds)': None, 'Memory (MB)': None}

# ğŸï¸ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Dask

def dask_processing(file_name):
    client = None
    try:
        # âš™ï¸ Ø¶Ø¨Ø· Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¹Ù…Ø§Ù„ Ù„ØªØ­Ù…ÙŠÙ„ Ø£ÙƒØ¨Ø± Ù„Ù„Ø°Ø§ÙƒØ±Ø©
        client = Client(n_workers=1,threads_per_worker=1, memory_target_fraction=0.9,memory_spill_fraction=0.9)
        
        # ğŸ“¦ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ø§Ù„Ø¥Ø¨Ù‚Ø§Ø¡ Ø¹Ù„ÙŠÙ‡Ø§ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        start_mem = measure_memory(repeats=10, delay=0.5)
        start_time = time.time()
        
        df = dd.read_csv(file_name,low_memory=False,blocksize='8MB').persist()  # Ø¥Ø¬Ø¨Ø§Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        
        # ğŸ”„ Ø¹Ù…Ù„ÙŠØ§Øª Ù…ØªÙƒØ±Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        for _ in range(3):  # ØªÙƒØ±Ø§Ø± Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø­Ù…Ù„
            df['price'] = df['price'].astype(float)
            df['discounted_price'] = df['price'] * 0.8  # Ø¹Ù…Ù„ÙŠØ§Øª Ø­Ø³Ø§Ø¨ÙŠØ© Ø¥Ø¶Ø§ÙÙŠØ©
            
        # ğŸ“Š Ø¹Ù…Ù„ÙŠØ§Øª ØªØ¬Ù…ÙŠØ¹ Ù…Ø¹Ù‚Ø¯Ø©
        grouped = df.groupby("event_type").agg(avg_price=('price', 'mean'),max_price=('price', 'max'),min_price=('price', 'min')).persist()  # Ø§Ù„Ø¥Ø¨Ù‚Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        
        # â³ ØªØ£Ø®ÙŠØ± Ù‚ÙŠØ§Ø³ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        time.sleep(10)  # Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ù„Ø¶Ù…Ø§Ù† Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„Ù‚ÙŠØ§Ø³
        end_mem = measure_memory(repeats=20, delay=1)
        
        return {
            'Method': 'Dask Processing','Time (Seconds)': round(time.time() - start_time, 2),'Memory (MB)': round(end_mem - start_mem, 2)}
    except Exception as e:
        print(f"âŒ ÙØ´Ù„ ÙÙŠ Dask Processing: {e}")
        return {'Method': 'Dask Processing', 'Time (Seconds)': None, 'Memory (MB)': None}
    finally:
        if client:
            client.close()

# ğŸ“‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Parquet
def parquet_compression(file_name):
    start_mem = measure_memory()
    start_time = time.time()

    try:
        # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù„Ù‰ Ø¯ÙØ¹Ø§Øª
        parquet_file = pq.ParquetFile(file_name)
        total_sum = {}
        total_count = {}
        
        for batch in parquet_file.iter_batches(columns=['event_type', 'price'], batch_size=100000):
            df_batch = batch.to_pandas()
            grouped = df_batch.groupby("event_type")["price"]
            for name, group in grouped:
                total_sum[name] = total_sum.get(name, 0) + group.sum()
                total_count[name] = total_count.get(name, 0) + group.count()
        
        avg_price = {k: total_sum[k]/total_count[k] for k in total_sum}
        
        return {
            'Method': 'Parquet Compression',
            'Time (Seconds)': round(time.time() - start_time, 2),
            'Memory (MB)': round(measure_memory() - start_mem, 2)
        }
    except Exception as e:
        print(f"âŒ ÙØ´Ù„ Ù‚Ø±Ø§Ø¡Ø© Parquet: {e}")
        return {'Method': 'Parquet Compression', 'Time (Seconds)': None, 'Memory (MB)': None}

# ğŸ”¬ Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙˆØ­ÙØ¸Ù‡Ø§
def compare_results(results, output_file="comparison_results.csv"):
    df = pd.DataFrame(results).dropna()
    
    print("\nğŸ” **Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©:**")
    if not df.empty:
        print(f"ğŸš€ Ø§Ù„Ø£Ø³Ø±Ø¹: {df.loc[df['Time (Seconds)'].idxmin(), 'Method']}")
        print(f"ğŸ’¾ Ø§Ù„Ø£Ù‚Ù„ Ø§Ø³ØªÙ‡Ù„Ø§ÙƒÙ‹Ø§ Ù„Ù„Ø°Ø§ÙƒØ±Ø©: {df.loc[df['Memory (MB)'].idxmin(), 'Method']}")
    else:
        print("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ ØµØ§Ù„Ø­Ø© Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©!")
    
    print("\nğŸ“Š **Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬:**")
    print(df.to_string(index=False))
    
    df.to_csv(output_file, index=False)
    print(f"\nâœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ: {output_file}")

# ğŸ Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
if __name__ == "__main__":
    base_path = r"G:\tpS2\big data\tp2"
    input_csv = os.path.join(base_path, "2019-Nov.csv")
    output_parquet = os.path.join(base_path, "2019-Nov.parquet")
    
    # âœ… ØªØ­ÙˆÙŠÙ„ CSV Ø¥Ù„Ù‰ Parquet
    success = convert_to_parquet(input_csv, output_parquet)
    
    if success:
        # ğŸï¸ ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª
        results = [
            pandas_chunking(input_csv),
            dask_processing(input_csv),
            parquet_compression(output_parquet)
        ]
        # ğŸ“Š Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        compare_results(results)
    else:
        print("âŒ ÙØ´Ù„ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ù„ÙØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª!")
